{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6205823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shill\\AppData\\Local\\Temp\\ipykernel_88436\\2351069235.py:7: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  url = pd.read_csv(\"../data/urls.csv\", encoding='latin1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shape: (822010, 17) b shape: (822010,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "url = pd.read_csv(\"../data/urls.csv\", encoding='latin1')\n",
    "\n",
    "# Clean boolean column\n",
    "url[\"has_suspicious_word\"] = url[\"has_suspicious_word\"].map(\n",
    "    {True: 1, False: 0, \"True\": 1, \"False\": 0}\n",
    ")\n",
    "\n",
    "# Feature columns\n",
    "feature_cols = [\n",
    "    'url_length','num_digits','digit_ratio','special_char_ratio','num_hyphens',\n",
    "    'num_underscores','num_slashes','num_dots','num_question_marks',\n",
    "    'num_equals','num_at_symbols','num_percent','num_hashes','num_ampersands',\n",
    "    'num_subdomains','is_https','has_suspicious_word'\n",
    "]\n",
    "\n",
    "url_features = url[feature_cols].astype(float)\n",
    "url_response = url['status'].fillna(0).astype(int)\n",
    "\n",
    "# Standardize\n",
    "x_mean = url_features.mean(axis=0)\n",
    "x_std_vec = url_features.std(axis=0) + 1e-8\n",
    "x_std = (url_features - x_mean) / x_std_vec\n",
    "\n",
    "# Drop rows with NaNs\n",
    "mask = x_std.notna().all(axis=1) & url_response.notna()\n",
    "\n",
    "A = x_std[mask].values\n",
    "b = url_response[mask].values.astype(float)\n",
    "\n",
    "print(\"A shape:\", A.shape, \"b shape:\", b.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433b10e0",
   "metadata": {},
   "source": [
    "$$\\text{Sigmoid function for logistic regression: }\\sigma = \\frac{1}{(1+ e^{-z})}$$\n",
    "$$\\text{Logistic Loss(using negative log likelihood): }f(x) = -\\sum_i [b_i \\log(1-p_i) + (1-b_i) \\log(1-p_i)] + (\\lambda \\ 2) ||x||^2$$\n",
    "\n",
    "We will use batch, mini-batch, and Newton's method for our algorithms using the above function as out minimizer. Due to the nature of the logistic regression model classifying 1 or 0, we can use that as out data is perfectly set up. The negative log likelihood function is a convex function which means that we can rely on these methods to find a global minimizer instead of stopping at a local mimimizer. We will compare the convergance and accuracy to asses the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a19e9ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def logistic_loss(w, b0, A, b, lam=0.0):\n",
    "    z = A @ w + b0\n",
    "    p = sigmoid(z)\n",
    "    eps = 1e-12\n",
    "    n = len(b)\n",
    "    nll = -np.mean(b * np.log(p + eps) + (1 - b) * np.log(1 - p + eps))\n",
    "    reg = 0.5 * lam * np.sum(w**2)\n",
    "    return nll + reg\n",
    "\n",
    "def logistic_grad(w, b0, A, b, lam=0.0):\n",
    "    n = A.shape[0]\n",
    "    z = A @ w + b0\n",
    "    p = sigmoid(z)\n",
    "    grad_w = A.T @ (p - b) / n + lam * w\n",
    "    grad_b = float(np.mean(p - b))\n",
    "    return grad_w, grad_b\n",
    "\n",
    "def predict_proba(A, w, b0):\n",
    "    return sigmoid(A @ w)\n",
    "\n",
    "def predict_label(A, w, b0, threshold=0.5):\n",
    "    return (predict_proba(A, w, b0) >= threshold).astype(int)\n",
    "\n",
    "def accuracy(A, b, w, b0):\n",
    "    return float((predict_label(A, w, b0) == b).mean())\n",
    "\n",
    "def logistic_hessian_w(w, b0, A, b, lam=0.0):\n",
    "    \"\"\"\n",
    "    Hessian of the logistic loss w.r.t. w only (d x d matrix).\n",
    "\n",
    "    H = (1/n) * A^T diag(p(1-p)) A + lam * I\n",
    "    \"\"\"\n",
    "    n, d = A.shape\n",
    "    z = A @ w + b0\n",
    "    p = sigmoid(z)\n",
    "    s = p * (1 - p)          # shape (n,)\n",
    "\n",
    "    # Each row of A scaled by s_i\n",
    "    AS = A * s[:, None]      # shape (n, d)\n",
    "\n",
    "    H = (AS.T @ A) / n       # (d, d)\n",
    "    H += lam * np.eye(d)\n",
    "    return H\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "75b38f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "def gd_step(w, b0, A, b, lr, lam=0.0):\n",
    "    grad_w, grad_b = logistic_grad(w, b0, A, b, lam)\n",
    "    return w - lr * grad_w, b0 - lr * grad_b\n",
    "\n",
    "def fit_batch_gd(A, b, lr=0.1, n_iters=40, lam=0.0):\n",
    "    n, d = A.shape\n",
    "    w = np.zeros(d)\n",
    "    b0 = 0.0\n",
    "    loss_hist = []\n",
    "    acc_hist = []\n",
    "    for k in range(n_iters):\n",
    "        w, b0 = gd_step(w, b0, A, b, lr, lam)\n",
    "        loss_hist.append(logistic_loss(w, b0, A, b, lam))\n",
    "        acc_hist.append(accuracy(A, b, w, b0))\n",
    "    return w, b0, loss_hist, acc_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c4bf4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch SGD\n",
    "def make_minibatches(n, batch_size, rng):\n",
    "    perm = rng.permutation(n)\n",
    "    return [perm[i:i+batch_size] for i in range(0, n, batch_size)]\n",
    "\n",
    "def sgd_minibatch_step(w, b0, A_batch, b_batch, lr, lam=0.0):\n",
    "    grad_w, grad_b = logistic_grad(w, b0, A_batch, b_batch, lam)\n",
    "    return w - lr * grad_w, b0 - lr * grad_b\n",
    "\n",
    "def fit_minibatch_sgd(A, b, lr=0.05, n_epochs=10, batch_size=512, lam=0.0, seed=0):\n",
    "    n, d = A.shape\n",
    "    w = np.zeros(d)\n",
    "    b0 = 0.0\n",
    "    rng = np.random.default_rng(seed)\n",
    "    loss_hist = []\n",
    "    acc_hist = []\n",
    "    for epoch in range(n_epochs):\n",
    "        batches = make_minibatches(n, batch_size, rng)\n",
    "        for idx in batches:\n",
    "            w, b0 = sgd_minibatch_step(w, b0, A[idx], b[idx], lr, lam)\n",
    "        loss_hist.append(logistic_loss(w, b0, A, b, lam))\n",
    "        acc_hist.append(accuracy(A, b, w, b0))\n",
    "    return w, b0, loss_hist, acc_hist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "93d02db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton's Method\n",
    "def newton_step(w, b0, A, b, lam=0.0):\n",
    "    \"\"\"\n",
    "    Perform one Newton step for (w, b0).\n",
    "    Uses gradient and Hessian for w, scalar Hessian for b0.\n",
    "    \"\"\"\n",
    "    n, d = A.shape\n",
    "\n",
    "    # gradient\n",
    "    grad_w, grad_b = logistic_grad(w, b0, A, b, lam)\n",
    "\n",
    "    # Hessian for w\n",
    "    H = logistic_hessian_w(w, b0, A, b, lam)\n",
    "\n",
    "    # Solve H * delta_w = grad_w\n",
    "    try:\n",
    "        delta_w = np.linalg.solve(H, grad_w)\n",
    "    except np.linalg.LinAlgError:\n",
    "        # If Hessian is singular, skip Newton update\n",
    "        delta_w = grad_w  # fall back to gradient step\n",
    "    w_new = w - delta_w\n",
    "\n",
    "    # For b0, scalar second derivative:\n",
    "    z = A @ w + b0\n",
    "    p = sigmoid(z)\n",
    "    s = p * (1 - p)\n",
    "    H_b = float(np.mean(s))  # approximate d^2 f / db0^2\n",
    "\n",
    "    if H_b > 0:\n",
    "        delta_b = grad_b / H_b\n",
    "        b0_new = b0 - delta_b\n",
    "    else:\n",
    "        b0_new = b0  # no update if curvature is degenerate\n",
    "\n",
    "    return w_new, b0_new\n",
    "\n",
    "def fit_newton(A, b, n_iters=10, lam=0.0):\n",
    "    \"\"\"\n",
    "    Train logistic regression with Newton's method.\n",
    "    Returns (w, b0, loss_history, acc_history).\n",
    "    \"\"\"\n",
    "    n, d = A.shape\n",
    "    w = np.zeros(d)\n",
    "    b0 = 0.0\n",
    "\n",
    "    loss_hist = []\n",
    "    acc_hist  = []\n",
    "\n",
    "    for k in range(n_iters):\n",
    "        w, b0 = newton_step(w, b0, A, b, lam)\n",
    "        loss_hist.append(logistic_loss(w, b0, A, b, lam))\n",
    "        acc_hist.append(accuracy(A, b, w, b0))\n",
    "\n",
    "    return w, b0, loss_hist, acc_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52541396",
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m         results \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m futures]\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m---> 30\u001b[0m results \u001b[38;5;241m=\u001b[39m run_all_methods(A, b, lam\u001b[38;5;241m=\u001b[39mlam)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Unpack into your existing names\u001b[39;00m\n\u001b[0;32m     33\u001b[0m loss_gd \u001b[38;5;241m=\u001b[39m acc_gd \u001b[38;5;241m=\u001b[39m loss_sgd \u001b[38;5;241m=\u001b[39m acc_sgd \u001b[38;5;241m=\u001b[39m loss_new \u001b[38;5;241m=\u001b[39m acc_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[89], line 27\u001b[0m, in \u001b[0;36mrun_all_methods\u001b[1;34m(A, b, lam)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m     22\u001b[0m     futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     23\u001b[0m         ex\u001b[38;5;241m.\u001b[39msubmit(train_gd, A, b, lam),\n\u001b[0;32m     24\u001b[0m         ex\u001b[38;5;241m.\u001b[39msubmit(train_sgd, A, b, lam),\n\u001b[0;32m     25\u001b[0m         ex\u001b[38;5;241m.\u001b[39msubmit(train_newton, A, b, lam),\n\u001b[0;32m     26\u001b[0m     ]\n\u001b[1;32m---> 27\u001b[0m     results \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m futures]\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\shill\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\shill\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "\n",
    "# Train methods\n",
    "lam = 0.0\n",
    "x0 = np.zeros(A.shape[1])\n",
    "\n",
    "w_gd, b_gd, loss_gd, acc_gd = fit_batch_gd(A, b, lr=0.1, n_iters=500, lam=lam)\n",
    "w_sgd, b_sgd, loss_sgd, acc_sgd = fit_minibatch_sgd(A, b, lr=0.05, n_epochs=500, batch_size=1024, lam=lam)\n",
    "w_new, b_new, loss_new, acc_new = fit_newton(A, b, n_iters=500, lam=lam)\n",
    "\n",
    "print(\"Batch GD final accuracy:    \", acc_gd[-1])\n",
    "print(\"Mini-batch SGD final acc:   \", acc_sgd[-1])\n",
    "print(\"Newton final acc:           \", acc_new[-1])\n",
    "\n",
    "# Loss plot \n",
    "plt.figure()\n",
    "plt.plot(loss_gd,   label=\"Batch GD\")\n",
    "plt.plot(loss_sgd,  label=\"Mini-batch SGD (per epoch)\")\n",
    "plt.plot(loss_new,  label=\"Newton\")\n",
    "plt.title(\"Training Loss: GD vs SGD vs Newton\")\n",
    "plt.xlabel(\"Iteration / Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"../img/loss_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.figure()\n",
    "plt.plot(acc_gd,   label=\"Batch GD\")\n",
    "plt.plot(acc_sgd,  label=\"Mini-batch SGD (per epoch)\")\n",
    "plt.plot(acc_new,  label=\"Newton\")\n",
    "plt.title(\"Training Accuracy: GD vs SGD vs Newton\")\n",
    "plt.xlabel(\"Iteration / Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(\"../img/accuracy_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
