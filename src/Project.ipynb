{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaed440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433b10e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Elementwise logistic function σ(z) = 1 / (1 + e^{-z}).\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fabade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value(A, b, x, lam=0.0):\n",
    "    r\"\"\"\n",
    "    Logistic regression objective with L2 penalty:\n",
    "\n",
    "        f(x) = -∑_i [ b_i log(p_i) + (1 - b_i) log(1 - p_i) ] + (lam/2) ||x||^2\n",
    "\n",
    "    where p_i = sigmoid(a_i^T x),  b_i ∈ {0,1}.\n",
    "    \"\"\"\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    b = np.asarray(b, dtype=float)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    z = A @ x                  # shape (n,)\n",
    "    p = sigmoid(z)             # predicted probabilities\n",
    "\n",
    "    eps = 1e-12                # numerical safety\n",
    "    nll = -(b * np.log(p + eps) + (1 - b) * np.log(1 - p + eps)).sum()\n",
    "    reg = 0.5 * lam * (x @ x)\n",
    "\n",
    "    return nll + reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(A,b,x):\n",
    "    \"\"\"r(x) = A x - b\"\"\"\n",
    "    return A @ x - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d825a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(A, b, x, lam=0.0):\n",
    "    r\"\"\"\n",
    "    Gradient of the logistic objective:\n",
    "\n",
    "        ∇f(x) = A^T (p - b) + lam * x\n",
    "\n",
    "    where p = sigmoid(Ax),  b ∈ {0,1}.\n",
    "    \"\"\"\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    b = np.asarray(b, dtype=float)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    z = A @ x\n",
    "    p = sigmoid(z)\n",
    "\n",
    "    return A.T @ (p - b) + lam * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262768be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepestDescentLogReg(\n",
    "        A, b, x0, lam=0.0, mode=\"armijo\", alpha_fixed=1e-2,\n",
    "        armijo_c=1e-4, rho=0.5, tol=1e-8, maxit=10_000\n",
    "):\n",
    "    \"\"\"\n",
    "    Steepest descent for logistic regression with L2 penalty.\n",
    "\n",
    "    Minimizes\n",
    "        f(x) = -∑ [b_i log p_i + (1 - b_i) log(1 - p_i)] + (lam/2)||x||^2,\n",
    "    where p = sigmoid(Ax), b ∈ {0,1}.\n",
    "\n",
    "    mode: \"fixed\" | \"armijo\"\n",
    "      - \"fixed\"  : use a constant step size alpha_fixed.\n",
    "      - \"armijo\" : backtracking line search with Armijo condition.\n",
    "    \"\"\"\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    b = np.asarray(b, dtype=float)\n",
    "    x = np.asarray(x0, dtype=float).copy()\n",
    "\n",
    "    fx = value(A, b, x, lam=lam)\n",
    "\n",
    "    for _ in range(maxit):\n",
    "        g = gradient(A, b, x, lam=lam)\n",
    "        gnorm = np.linalg.norm(g)\n",
    "\n",
    "        if gnorm < tol:\n",
    "            break\n",
    "\n",
    "        if mode == \"fixed\":\n",
    "            alpha = alpha_fixed\n",
    "\n",
    "        elif mode == \"armijo\":\n",
    "            alpha = 1.0\n",
    "            # Armijo condition: f(x - αg) ≤ f(x) - c α ||g||²\n",
    "            while value(A, b, x - alpha * g, lam=lam) > fx - armijo_c * alpha * (gnorm ** 2):\n",
    "                alpha *= rho\n",
    "                if alpha < 1e-16:\n",
    "                    # step size underflow; treat as convergence\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            raise ValueError('mode must be \"fixed\" or \"armijo\"')\n",
    "\n",
    "        # gradient step\n",
    "        x = x - alpha * g\n",
    "        fx = value(A, b, x, lam=lam)\n",
    "\n",
    "    return x, fx"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
